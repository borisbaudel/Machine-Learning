{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f3da049",
   "metadata": {},
   "source": [
    "# Goals\n",
    "\n",
    "Understand the basic principles underlying:\n",
    "\n",
    "- Classification\n",
    "- Multilayer perceptrons\n",
    "- Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4367abaa",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "The most important goal is to understand.\n",
    "\n",
    "At each part of the notebook, you will find hyper-parameters. Your task is to change these hyper-parameters and study their effect.\n",
    "\n",
    "1) The dataset\n",
    "\n",
    "Visualize the data and understand the task.\n",
    "\n",
    "Hyper-parameters to change: \n",
    "- size of the dataset (n_samples)\n",
    "- shape of the circles (factor)\n",
    "- strength of the noise (noise)\n",
    "- number of examples per batch (batch_size)\n",
    "\n",
    "\n",
    "2) The network\n",
    "\n",
    "If you are in \"advanced coding mode\", fill in the gaps.\n",
    "\n",
    "Hyperparameters to change:\n",
    "- the size of the hidden layer (dim_h)\n",
    "- the scale of the initial random weights (w)\n",
    "\n",
    "3) Training and testing the network\n",
    "\n",
    "\n",
    "Hyper-parameters to change: \n",
    "- number of epochs (n_epochs)\n",
    "- learning rate (learning_rate)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca0ac82",
   "metadata": {},
   "source": [
    "# 0) Loading useful packages and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31367205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # to handle operations on arrays of numbers, similar to matlab\n",
    "import matplotlib.pyplot as plt  # to plot figures\n",
    "import matplotlib.cm as cm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import sklearn as skl\n",
    "import load_dataset as load  # home-made module with functions to load datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c5fece",
   "metadata": {},
   "source": [
    "# 1) The dataset\n",
    "\n",
    "We load the \"circles\" dataset.\n",
    "\n",
    "For now, we set a batch_size of 1, meaning we will give the examples one by one to the network.\n",
    "\n",
    "The loader gives us a train_loader and a test_loader, having 75 % and 25 % of the dataset examples respectively.\n",
    "\n",
    "You can choose different \"factor\" values between the circles, and different \"noise\" levels.\n",
    "\n",
    "If your computer is slow, you can reduce the number of examples n_samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6223a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = ...  # the number of examples per batch\n",
    "train_loader, test_loader, dim_in, dim_out = load.load_circles(batch_size=batch_size, \n",
    "                                                               n_samples=..., \n",
    "                                                               shuffle=True, \n",
    "                                                               noise=..., \n",
    "                                                               factor=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29937ba0",
   "metadata": {},
   "source": [
    "Let's look into the dataset.\n",
    "\n",
    "We can plot it and look at some examples of inputs and targets.\n",
    "\n",
    "Note that the inputs are of the class \"tensor\" from pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538d2297",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_loader.dataset.data  # we take the DATA (here all dataset)\n",
    "targets = train_loader.dataset.targets  # we take the TARGET (here all dataset)\n",
    "ax = plt.subplot()\n",
    "ax.scatter(X[:, 0], X[:, 1], c=targets, cmap=cm.coolwarm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc66711",
   "metadata": {},
   "source": [
    "# 2) The network\n",
    "\n",
    "We first code useful functions, then assemble them into a network.\n",
    "\n",
    "We chose the ReLU as the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ad8a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    y = x.clone()\n",
    "    y[x <0] = ...\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8a1359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_relu(x):\n",
    "    y = torch.ones_like(x)\n",
    "    y[x<0]=...\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d21f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    z = torch.exp(y)/(torch.exp(y).sum(axis=-1)).unsqueeze(dim=-1)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4734b3",
   "metadata": {},
   "source": [
    "Here we define the network. A multi layer perceptron with one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a94b7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_layer_perceptron():\n",
    "    def __init__(self, dim_in, dim_h, dim_out, w):\n",
    "        self.dim_in = dim_in # number of inputs\n",
    "        self.dim_h = dim_h # number of hidden neurons\n",
    "        self.dim_out = dim_out # number of outputs\n",
    "        \n",
    "        # Here we define the parameters (weights and biases) and initialize them randomly\n",
    "        # Help: to create a random array with normal distribution of stdv 1: torch.randn(dim0, dim1)\n",
    "        self.weights_1 = torch.randn(..., ...) * w\n",
    "        self.bias_1 = torch.randn(...) * w\n",
    "        self.weights_2 = torch.randn(..., ...) * w\n",
    "        self.bias_2 = torch.randn(...) * w\n",
    "        \n",
    "    # The forward function is what the network does: what transformations are applied to the inputs.\n",
    "    # Help: tensor multiplication torch.matmul(tensor1, tensor2)\n",
    "    def forward(self, x):\n",
    "        self.y_1 = ...\n",
    "        self.h = ...\n",
    "        y_2 = ...\n",
    "        return y_2\n",
    "    \n",
    "    # This function computes the gradients associated to each parameter\n",
    "    def compute_grad(self, x, targets):\n",
    "        y_2 = ... # we apply the network\n",
    "        p = ...\n",
    "        targets = torch.nn.functional.one_hot(targets, num_classes=2).float()\n",
    "        loss = ... # cross entropy loss. Help: dot product torch.dot(vec1, vec2)\n",
    "        \n",
    "        #Layer 2 of weights and biases. Initialize arrays.\n",
    "        #Initialize arrays.\n",
    "        self.dL_db2 = torch.zeros_like(self.bias_2)\n",
    "        self.dL_dw2 = torch.zeros_like(self.weights_2)\n",
    "\n",
    "        for j in range(self.dim_out):\n",
    "            for i in range(self.dim_h):\n",
    "                self.dL_dw2[i, j] = ...\n",
    "            self.dL_db2[j] = ...\n",
    "\n",
    "        #Layer 1 of weights and biases\n",
    "        #Initialize arrays.\n",
    "        self.dL_db1 = torch.zeros_like(self.bias_1)\n",
    "        self.dL_dw1 = torch.zeros_like(self.weights_1)\n",
    "        \n",
    "        for j in range(self.dim_h):\n",
    "            for i in range(self.dim_in):\n",
    "                self.dL_dw1[i, j] = ...\n",
    "            self.dL_db1[j] = ...\n",
    "            \n",
    "        return loss\n",
    "            \n",
    "    # This function updates the parameters using the gradients\n",
    "    def update_params(self, learning_rate):\n",
    "        self.weights_1 = ...\n",
    "        self.bias_1 = ...\n",
    "        self.weights_2 = ...\n",
    "        self.bias_2 = ...\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d71970",
   "metadata": {},
   "source": [
    "We make a function to plot the results of the network on the task/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803ff84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(network, x, targets):\n",
    "    y = network.forward(x)\n",
    "    p = softmax(y)\n",
    "    pred = y.argmax(dim=1) # the predicted class is the output neurons with the highest value\n",
    "    idxs = torch.nonzero(((pred == targets[:]) == False)).squeeze() # indexes predicted in wrong class\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    ax[0].scatter(x[:, 0], x[:, 1], c=p[:,1], cmap=cm.coolwarm, vmin=0, vmax=1) # proba to be class 1\n",
    "    ax[0].set_title('Probability to be class 1')\n",
    "    ax[1].scatter(x[:, 0], x[:, 1], c=pred, cmap=cm.coolwarm, vmin=0, vmax=1) # class\n",
    "    ax[1].scatter(x[idxs, 0], x[idxs, 1], facecolors='none', edgecolor='green', linewidths=2) # misclassified\n",
    "    ax[1].set_title('Predicted class (red = 1)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58990428",
   "metadata": {},
   "source": [
    "We make a function to compute the accuracy on the task, i.e. the proportion of correctly classified examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edddb4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(network, x, targets):\n",
    "    y = network.forward(x)\n",
    "    p = softmax(y)\n",
    "    pred = p.argmax(dim=-1)\n",
    "    accuracy = (pred == targets).double().mean()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07d834a",
   "metadata": {},
   "source": [
    "We create a network suitable to solve the circles task. \n",
    "\n",
    "Hyperparameters to change:\n",
    "- the size of the hidden layer (dim_h)\n",
    "- the scale of the initial random weights (w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ef5e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = multi_layer_perceptron(dim_in = ..., dim_h = ..., dim_out = ..., w=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9826ce",
   "metadata": {},
   "source": [
    "# 3) Training and testing the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc220ee1",
   "metadata": {},
   "source": [
    "Let's test the network before training. We observe that the results are poor: this is random guessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d731082",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_X_test = test_loader.dataset.data\n",
    "all_targets_test = test_loader.dataset.targets\n",
    "\n",
    "plot_results(net, all_X_test, all_targets_test)\n",
    "print(f'accuracy is {accuracy(net, all_X_test, all_targets_test)*100:.2f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b174a5",
   "metadata": {},
   "source": [
    "Now let's train the network and track the evolution of the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bfd717",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "test_accuracy_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8613749",
   "metadata": {},
   "source": [
    "Here choose the learning rate and number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8f53eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = ...\n",
    "n_epochs = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e05a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    loss = 0\n",
    "    for idx in range(len(train_loader)):\n",
    "        data = train_loader.dataset.data[idx]\n",
    "        target = train_loader.dataset.targets[idx]\n",
    "        loss += net.compute_grad(data, target)/len(train_loader)\n",
    "        net.update_params(learning_rate = learning_rate)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    test_acc = accuracy(net, all_X_test, all_targets_test)*100\n",
    "    test_accuracy_list.append(test_acc)\n",
    "    plot_results(net, all_X_test, all_targets_test)\n",
    "    plt.show()\n",
    "    print(f'Epoch {epoch} loss: {loss}, accuracy: {test_acc:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd3ad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('plot_style.mplstyle')  # A file with parameters on what you want the figure to look like\n",
    "fig, ax = plt.subplots(1, 2)  # a figure with 1 row and 2 columns of subplots\n",
    "# fig is the figure itself, ax is an array of two independent subplots inside fig\n",
    "# on the left side, plot the losses. \n",
    "ax[0].plot(train_loss_list, c='red', ls='-')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Training Loss')\n",
    "# on the right side, plot the accuracies. \n",
    "ax[1].plot(test_accuracy_list, c='red')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Test Accuracy (%)')\n",
    "# We put a title for our figure.\n",
    "fig_title = f'Training a perceptron'\n",
    "fig.suptitle(fig_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20422533",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(net, all_X_test, all_targets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57477e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
